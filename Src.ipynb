{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f23f18",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6920cf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openpyxl\\reader\\workbook.py:118: UserWarning: Print area cannot be set to Defined name: 'Wedding budget'!$A:$K.\n",
      "  warn(f\"Print area cannot be set to Defined name: {defn.value}.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "\n",
    "# Loading .doc files\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text).strip()\n",
    "\n",
    "# Loading .pdf files\n",
    "def read_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text_by_page = []\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            text_by_page.append((i + 1, text.strip()))\n",
    "    return text_by_page  # list of (page_number, text)\n",
    "\n",
    "# Loading .xls files\n",
    "def read_excel(file_path):\n",
    "    excel_data = pd.read_excel(file_path, sheet_name=None) \n",
    "    text = []\n",
    "    for sheet, df in excel_data.items():\n",
    "        text.append(f'--- Sheet: {sheet} ---\\n')\n",
    "        text.append(df.to_string(index=False))\n",
    "    return '\\n'.join(text).strip()\n",
    "\n",
    "# Loading .csv files\n",
    "def read_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df.to_string(index=False).strip()\n",
    "\n",
    "# Extracting Text in chunks from the files\n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == '.docx':\n",
    "        return {'filename': os.path.basename(file_path), 'text': read_docx(file_path)}\n",
    "    elif ext == '.pdf':\n",
    "        pages = read_pdf(file_path)\n",
    "        return [{'filename': os.path.basename(file_path), 'page': p, 'text': t} for p, t in pages]\n",
    "    elif ext in ['.xlsx', '.xls', '.xlsm']:\n",
    "        return {'filename': os.path.basename(file_path), 'text': read_excel(file_path)}\n",
    "    elif ext == '.csv':\n",
    "        return {'filename': os.path.basename(file_path), 'text': read_csv(file_path)}\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {ext}\")\n",
    "        return None\n",
    "\n",
    "folder_path = 'C:/Users/User/Downloads/Dr.X Files/Dataset'\n",
    "all_text_chunks = []\n",
    "\n",
    "for fname in os.listdir(folder_path):\n",
    "    fpath = os.path.join(folder_path, fname)\n",
    "    result = extract_text(fpath)\n",
    "    \n",
    "    if isinstance(result, list): \n",
    "        all_text_chunks.extend(result)\n",
    "    elif result:\n",
    "        all_text_chunks.append(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6af25f",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76d3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45af93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Dataset summaries and citations.docx, Page: None, Chunk: 1\n",
      "Table 1. Description of studies included in the meta-analysis. Full article citations are listed after the table.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Citation List\n",
      "Acuña E., A. A., Pastenes V., C., & Villalobos G., L. (2017). Ca ...\n",
      "\n",
      "File: Dataset summaries and citations.docx, Page: None, Chunk: 2\n",
      "https://doi.org/10.3390/f5030425\n",
      "Carley, D. S., Goodman, D., Sermons, S., Shi, W., Bowman, D., Miller, G., & Rufty, T. (2011). Soil Organic Matter Accumulation in Creeping Bentgrass Greens: A Chronose ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, max_tokens=300, overlap=50):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk_tokens = tokens[i:i+max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text.strip())\n",
    "        i += max_tokens - overlap  # Slide window\n",
    "    return chunks\n",
    "\n",
    "def chunk_documents(parsed_docs, max_tokens=300):\n",
    "    chunked_data = []\n",
    "\n",
    "    for doc in parsed_docs:\n",
    "        filename = doc['filename']\n",
    "        page = doc.get('page', None)\n",
    "        text = doc['text']\n",
    "\n",
    "        chunks = chunk_text(text, max_tokens=max_tokens)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunked_data.append({\n",
    "                'filename': filename,\n",
    "                'page': page,\n",
    "                'chunk_number': idx + 1,\n",
    "                'text': chunk\n",
    "            })\n",
    "\n",
    "    return chunked_data\n",
    "\n",
    "chunked_docs = chunk_documents(all_text_chunks, max_tokens=300)\n",
    "\n",
    "# Printed sample to check chunked doc\n",
    "for entry in chunked_docs[:2]:\n",
    "    print(f\"File: {entry['filename']}, Page: {entry.get('page', 'N/A')}, Chunk: {entry['chunk_number']}\")\n",
    "    print(entry['text'][:200], '...\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6962134a",
   "metadata": {},
   "source": [
    "# Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e11ba8",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8f1e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "def embed_texts_local(texts):\n",
    "    return embed_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "texts = [doc['text'] for doc in chunked_docs]\n",
    "embeddings = embed_texts_local(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d1736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def build_faiss_index(chunks, dim=384):  # 384 is for MiniLM, we can adjust it for other models\n",
    "    texts = [chunk['text'] for chunk in chunks]\n",
    "    embeddings = embed_texts_local(texts)\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(embeddings).astype('float32'))\n",
    "    metadata = [\n",
    "        {\n",
    "            'filename': chunk['filename'],\n",
    "            'page': chunk.get('page', 'N/A'),\n",
    "            'chunk_number': chunk['chunk_number'],\n",
    "            'text': chunk['text']\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    return index, embeddings, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb8d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_index(index, metadata, path='Database/vector_index'):\n",
    "    faiss.write_index(index, f'{path}.index')\n",
    "    with open(f'{path}_meta.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42db0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index(path='Database/vector_index'):\n",
    "    index = faiss.read_index(f'{path}.index')\n",
    "    with open(f'{path}_meta.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    return index, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a2e0db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index, embeddings, metadata = build_faiss_index(chunked_docs)\n",
    "save_index(index, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e9b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, 'Database/index.index')\n",
    "with open('Database/index_meta.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46499446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_index(query, index, metadata, top_k=5):\n",
    "    query_embedding = embed_texts_local([query])[0]\n",
    "    D, I = index.search(np.array([query_embedding]).astype('float32'), top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        results.append(metadata[idx])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e061cb",
   "metadata": {},
   "source": [
    "# RAG Q&A System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c5d2cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, chunks):\n",
    "    context = \"\\n\\n\".join([f\"Source [{c['filename']}, page {c.get('page', 'N/A')}, chunk {c['chunk_number']}]:\\n{c['text']}\" for c in chunks])\n",
    "    prompt = f\"\"\"You are an intelligent assistant helping with scientific research.\n",
    "\n",
    "Answer the following question using only the information provided in the sources below.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Sources:\n",
    "{context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23caa418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q2_K - Medium\n",
      "print_info: file size   = 2.63 GiB (3.35 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q2_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  2694.32 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "# Adjust path to your GGUF model\n",
    "llm = Llama(model_path=\"llama-2-7b-chat.Q2_K.gguf\", n_ctx=2048, n_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe9709c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, index, metadata, top_k=5):\n",
    "    top_chunks = search_index(question, index, metadata, top_k=top_k)\n",
    "    prompt = build_prompt(question, top_chunks)\n",
    "\n",
    "    output = llm(prompt, max_tokens=300, stop=[\"User:\", \"Question:\"], echo=False)\n",
    "    return output['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47baeb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  191328.92 ms\n",
      "llama_perf_context_print: prompt eval time =  191327.12 ms /  1834 tokens (  104.32 ms per token,     9.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2994.34 ms /    16 runs   (  187.15 ms per token,     5.34 tokens per second)\n",
      "llama_perf_context_print:       total time =  194332.52 ms /  1850 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The total photography budget in a party is $2950.\n"
     ]
    }
   ],
   "source": [
    "index, metadata = load_index()\n",
    "query = \"What is the total photography budget in a party?\"\n",
    "response = answer_question(query, index, metadata)\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4908003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 33 prefix-match hit, remaining 1727 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  191328.92 ms\n",
      "llama_perf_context_print: prompt eval time =  185752.89 ms /  1727 tokens (  107.56 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24138.57 ms /   132 runs   (  182.87 ms per token,     5.47 tokens per second)\n",
      "llama_perf_context_print:       total time =  209996.09 ms /  1859 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Amino acids can be classified into two categories with regard to carbon metabolism: essential and non-essential. Essential amino acids are those that cannot be synthesized by an organism from materials normally available to the cells at a speed adequate with the demands for normal growth, while non-essential amino acids are those that can be synthesized from a variety of precursors in the Krebs cycle and other metabolic pathways. The categories are based on the degree of δ13C fractionation between the diet and consumer during nitrogen metabolism.\n"
     ]
    }
   ],
   "source": [
    "index, metadata = load_index()\n",
    "query = \"What are the categories of amino acids with regard to carbon metabolism?\"\n",
    "response = answer_question(query, index, metadata)\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60add1d6",
   "metadata": {},
   "source": [
    "# Translators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa4a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from langdetect import detect\n",
    "\n",
    "def load_translation_model(src_lang='ar', tgt_lang='en'):\n",
    "    model_name = f\"opus-mt-{src_lang}-{tgt_lang}\"\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def translate_text(text, model, tokenizer):\n",
    "    batch = tokenizer.prepare_seq2seq_batch([text], return_tensors=\"pt\")\n",
    "    generated = model.generate(**batch)\n",
    "    return tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "\n",
    "def translate_chunks(chunks, model, tokenizer):\n",
    "    translated_chunks = []\n",
    "    for c in chunks:\n",
    "        translated_text = translate_text(c['text'], model, tokenizer)\n",
    "        translated_chunks.append({**c, 'translated_text': translated_text})\n",
    "    return translated_chunks\n",
    "\n",
    "def translate_document_chunks_auto(chunks):\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        result = auto_translate(chunk['text'])\n",
    "        results.append({**chunk, 'translated_text': result['translated_text']})\n",
    "    return results\n",
    "\n",
    "# For auto language detection\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "    \n",
    "def load_model_by_lang(lang_code):\n",
    "    # Arabic → English\n",
    "    if lang_code == 'ar':\n",
    "        model_name = \"opus-mt-ar-en\"\n",
    "    # English → Arabic\n",
    "    elif lang_code == 'en':\n",
    "        model_name = \"opus-mt-en-ar\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {lang_code}\")\n",
    "    \n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "    \n",
    "def auto_translate(text):\n",
    "    lang = detect_language(text)\n",
    "    \n",
    "    if lang == 'ar':\n",
    "        target_lang = 'en'\n",
    "    elif lang == 'en':\n",
    "        target_lang = 'ar'\n",
    "    else:\n",
    "        raise ValueError(f\"Language {lang} not supported for auto-translation.\")\n",
    "\n",
    "    model, tokenizer = load_model_by_lang(lang)\n",
    "\n",
    "    batch = tokenizer.prepare_seq2seq_batch([text], return_tensors=\"pt\")\n",
    "    generated = model.generate(**batch)\n",
    "    translated = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "\n",
    "    return {\n",
    "        'original_lang': lang,\n",
    "        'target_lang': target_lang,\n",
    "        'translated_text': translated\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d82fea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:4106: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_lang': 'en', 'target_lang': 'ar', 'translated_text': 'د. (إكس) كان باحثاً مشهوراً'}\n",
      "{'original_lang': 'ar', 'target_lang': 'en', 'translated_text': 'Dr. X was a famous researcher.'}\n"
     ]
    }
   ],
   "source": [
    "# Testing Translators\n",
    "test1 = \"Dr. X was a well-known researcher.\"\n",
    "test2 = \"الدكتور إكس كان باحثًا مشهورًا.\"\n",
    "\n",
    "print(auto_translate(test1))  \n",
    "print(auto_translate(test2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4bbc63",
   "metadata": {},
   "source": [
    "# ROUGE metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "731e5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f04d94af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm  \n",
    "from collections import defaultdict\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def summarize_text_t5(text, max_length=130, min_length=30):\n",
    "    prompt = \"summarize: \" + text\n",
    "    return summarizer(prompt, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']\n",
    "\n",
    "def summarize_document_chunks(chunks):\n",
    "    summarized = []\n",
    "\n",
    "    for chunk in tqdm(chunks, desc=\"Summarizing chunks\"):\n",
    "        text = chunk['text']\n",
    "\n",
    "        try:\n",
    "            summary = summarize_text_t5(text)\n",
    "        except Exception as e:\n",
    "            summary = \"[Summary failed]\"\n",
    "            print(f\"Error on chunk {chunk['chunk_number']}: {e}\")\n",
    "\n",
    "        summarized.append({\n",
    "            **chunk,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "    return summarized\n",
    "\n",
    "def group_summaries_by_file(summarized_chunks):\n",
    "    file_summary_map = defaultdict(list)\n",
    "    for chunk in summarized_chunks:\n",
    "        file_summary_map[chunk['filename']].append(chunk['summary'])\n",
    "    return file_summary_map\n",
    "\n",
    "def generate_super_summary(summaries_list, max_length=150, min_length=40):\n",
    "    combined_summary = \" \".join(summaries_list)\n",
    "    return summarize_text_t5(combined_summary, max_length=max_length, min_length=min_length)\n",
    "\n",
    "def generate_super_summaries_per_file(summarized_chunks):\n",
    "    file_summaries = group_summaries_by_file(summarized_chunks)\n",
    "    super_summaries = {}\n",
    "\n",
    "    for file, summaries in tqdm(file_summaries.items(), desc=\"Generating super summaries\"):\n",
    "        try:\n",
    "            super_summary = generate_super_summary(summaries)\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing file {file}: {e}\")\n",
    "            super_summary = \"[Super-summary failed]\"\n",
    "        \n",
    "        super_summaries[file] = super_summary\n",
    "\n",
    "    return super_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23b74bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " X was a mysterious scientist who disappeared after publishing numerous groundbreaking papers . he was the subject of a series of groundbreaking papers that have been published .\n"
     ]
    }
   ],
   "source": [
    "long_text = \"Dr. X was a mysterious scientist who disappeared after publishing numerous groundbreaking papers...\"\n",
    "summary = summarize_text_t5(long_text)\n",
    "print(\"Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08f7bae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing chunks:   1%|          | 7/783 [00:08<15:38,  1.21s/it]Your max_length is set to 130, but your input_length is only 98. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n",
      "Summarizing chunks:   1%|          | 9/783 [00:10<14:29,  1.12s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Summarizing chunks:   2%|▏         | 12/783 [00:16<20:54,  1.63s/it]Your max_length is set to 130, but your input_length is only 67. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      "Summarizing chunks:   4%|▎         | 28/783 [00:47<22:21,  1.78s/it]Your max_length is set to 130, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Summarizing chunks:  16%|█▌        | 125/783 [02:44<11:23,  1.04s/it]Your max_length is set to 130, but your input_length is only 10. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
      "Summarizing chunks:  17%|█▋        | 130/783 [02:49<11:45,  1.08s/it]Your max_length is set to 130, but your input_length is only 66. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      "Summarizing chunks:  17%|█▋        | 135/783 [02:55<13:44,  1.27s/it]Your max_length is set to 130, but your input_length is only 129. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=64)\n",
      "Summarizing chunks:  19%|█▉        | 151/783 [03:19<17:57,  1.70s/it]Your max_length is set to 130, but your input_length is only 99. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n",
      "Summarizing chunks:  20%|█▉        | 156/783 [03:26<15:20,  1.47s/it]Your max_length is set to 130, but your input_length is only 43. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "Summarizing chunks:  22%|██▏       | 174/783 [03:45<10:15,  1.01s/it]Your max_length is set to 130, but your input_length is only 18. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n",
      "Summarizing chunks:  23%|██▎       | 182/783 [03:56<14:45,  1.47s/it]Your max_length is set to 130, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Summarizing chunks:  25%|██▍       | 194/783 [04:14<13:54,  1.42s/it]Your max_length is set to 130, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Summarizing chunks:  27%|██▋       | 213/783 [04:46<15:35,  1.64s/it]Your max_length is set to 130, but your input_length is only 80. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Summarizing chunks:  28%|██▊       | 223/783 [04:59<12:53,  1.38s/it]Your max_length is set to 130, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Summarizing chunks:  30%|██▉       | 232/783 [05:13<14:09,  1.54s/it]Your max_length is set to 130, but your input_length is only 62. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n",
      "Summarizing chunks:  30%|███       | 236/783 [05:18<13:47,  1.51s/it]Your max_length is set to 130, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Summarizing chunks:  31%|███       | 241/783 [05:28<16:23,  1.82s/it]Your max_length is set to 130, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Summarizing chunks:  33%|███▎      | 256/783 [05:54<14:01,  1.60s/it]Your max_length is set to 130, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Summarizing chunks:  34%|███▍      | 267/783 [06:11<13:49,  1.61s/it]Your max_length is set to 130, but your input_length is only 41. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n",
      "Summarizing chunks:  35%|███▍      | 272/783 [06:17<11:03,  1.30s/it]Your max_length is set to 130, but your input_length is only 103. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n",
      "Summarizing chunks:  35%|███▌      | 276/783 [06:23<11:39,  1.38s/it]Your max_length is set to 130, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n",
      "Summarizing chunks:  39%|███▊      | 303/783 [07:07<12:43,  1.59s/it]Your max_length is set to 130, but your input_length is only 120. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n",
      "Summarizing chunks:  41%|████      | 320/783 [07:29<08:53,  1.15s/it]Your max_length is set to 130, but your input_length is only 60. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
      "Summarizing chunks:  42%|████▏     | 327/783 [07:37<10:27,  1.38s/it]Your max_length is set to 130, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Summarizing chunks:  43%|████▎     | 334/783 [07:47<10:42,  1.43s/it]Your max_length is set to 130, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n",
      "Summarizing chunks:  45%|████▌     | 353/783 [08:10<08:43,  1.22s/it]Your max_length is set to 130, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "Summarizing chunks:  48%|████▊     | 372/783 [08:32<08:07,  1.19s/it]Your max_length is set to 130, but your input_length is only 105. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n",
      "Summarizing chunks:  49%|████▉     | 385/783 [08:49<08:40,  1.31s/it]Your max_length is set to 130, but your input_length is only 57. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Summarizing chunks:  51%|█████     | 399/783 [09:04<06:55,  1.08s/it]Your max_length is set to 130, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
      "Summarizing chunks:  53%|█████▎    | 412/783 [09:17<06:37,  1.07s/it]Your max_length is set to 130, but your input_length is only 35. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Summarizing chunks:  54%|█████▍    | 424/783 [09:32<06:51,  1.15s/it]Your max_length is set to 130, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Summarizing chunks:  55%|█████▍    | 428/783 [09:38<09:10,  1.55s/it]Your max_length is set to 130, but your input_length is only 71. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      "Summarizing chunks:  55%|█████▍    | 430/783 [09:41<09:41,  1.65s/it]Your max_length is set to 130, but your input_length is only 76. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n",
      "Summarizing chunks:  58%|█████▊    | 451/783 [10:08<05:56,  1.07s/it]Your max_length is set to 130, but your input_length is only 85. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Summarizing chunks:  58%|█████▊    | 454/783 [10:11<05:39,  1.03s/it]Your max_length is set to 130, but your input_length is only 125. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n",
      "Summarizing chunks:  58%|█████▊    | 456/783 [10:13<06:17,  1.15s/it]Your max_length is set to 130, but your input_length is only 92. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n",
      "Summarizing chunks:  59%|█████▉    | 462/783 [10:20<06:09,  1.15s/it]Your max_length is set to 130, but your input_length is only 68. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      "Summarizing chunks:  60%|█████▉    | 469/783 [10:29<06:46,  1.29s/it]Your max_length is set to 130, but your input_length is only 85. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Summarizing chunks:  61%|██████    | 474/783 [10:33<05:12,  1.01s/it]Your max_length is set to 130, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      "Summarizing chunks:  62%|██████▏   | 485/783 [10:44<05:03,  1.02s/it]Your max_length is set to 130, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Summarizing chunks:  62%|██████▏   | 488/783 [10:47<05:09,  1.05s/it]Your max_length is set to 130, but your input_length is only 35. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Summarizing chunks:  64%|██████▎   | 499/783 [11:00<05:48,  1.23s/it]Your max_length is set to 130, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Summarizing chunks:  65%|██████▍   | 506/783 [11:07<05:00,  1.09s/it]Your max_length is set to 130, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Summarizing chunks:  65%|██████▌   | 511/783 [11:12<04:27,  1.02it/s]Your max_length is set to 130, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Summarizing chunks:  66%|██████▋   | 520/783 [11:22<04:43,  1.08s/it]Your max_length is set to 130, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Summarizing chunks:  67%|██████▋   | 527/783 [11:29<04:47,  1.12s/it]Your max_length is set to 130, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "Summarizing chunks:  68%|██████▊   | 530/783 [11:32<04:39,  1.11s/it]Your max_length is set to 130, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
      "Summarizing chunks:  68%|██████▊   | 534/783 [11:36<04:38,  1.12s/it]Your max_length is set to 130, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "Summarizing chunks:  69%|██████▉   | 541/783 [11:43<03:52,  1.04it/s]Your max_length is set to 130, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Summarizing chunks:  69%|██████▉   | 544/783 [11:46<03:53,  1.02it/s]Your max_length is set to 130, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Summarizing chunks:  71%|███████   | 557/783 [12:00<04:26,  1.18s/it]Your max_length is set to 130, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Summarizing chunks:  72%|███████▏  | 566/783 [12:10<03:37,  1.00s/it]Your max_length is set to 130, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Summarizing chunks:  73%|███████▎  | 573/783 [12:17<03:39,  1.04s/it]Your max_length is set to 130, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Summarizing chunks:  74%|███████▍  | 580/783 [12:24<03:32,  1.05s/it]Your max_length is set to 130, but your input_length is only 15. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
      "Summarizing chunks:  75%|███████▍  | 585/783 [12:28<03:15,  1.01it/s]Your max_length is set to 130, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Summarizing chunks:  75%|███████▌  | 591/783 [12:35<03:42,  1.16s/it]Your max_length is set to 130, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n",
      "Summarizing chunks:  78%|███████▊  | 608/783 [12:58<04:22,  1.50s/it]Your max_length is set to 130, but your input_length is only 30. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Summarizing chunks:  78%|███████▊  | 611/783 [13:01<03:44,  1.30s/it]Your max_length is set to 130, but your input_length is only 69. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      "Summarizing chunks:  79%|███████▊  | 616/783 [13:07<03:30,  1.26s/it]Your max_length is set to 130, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
      "Summarizing chunks:  79%|███████▉  | 619/783 [13:11<03:40,  1.35s/it]Your max_length is set to 130, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Summarizing chunks:  80%|███████▉  | 623/783 [13:16<03:33,  1.34s/it]Your max_length is set to 130, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n",
      "Summarizing chunks:  80%|███████▉  | 626/783 [13:19<03:08,  1.20s/it]Your max_length is set to 130, but your input_length is only 16. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n",
      "Summarizing chunks:  80%|████████  | 629/783 [13:22<02:57,  1.15s/it]Your max_length is set to 130, but your input_length is only 27. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
      "Summarizing chunks:  81%|████████  | 631/783 [13:25<03:09,  1.25s/it]Your max_length is set to 130, but your input_length is only 103. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n",
      "Summarizing chunks:  81%|████████  | 634/783 [13:27<02:34,  1.04s/it]Your max_length is set to 130, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Summarizing chunks:  82%|████████▏ | 643/783 [13:37<02:49,  1.21s/it]Your max_length is set to 130, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "Summarizing chunks:  83%|████████▎ | 648/783 [13:43<02:51,  1.27s/it]Your max_length is set to 130, but your input_length is only 58. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Summarizing chunks:  83%|████████▎ | 653/783 [13:48<02:18,  1.06s/it]Your max_length is set to 130, but your input_length is only 32. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n",
      "Summarizing chunks:  84%|████████▎ | 655/783 [13:50<02:05,  1.02it/s]Your max_length is set to 130, but your input_length is only 125. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n",
      "Summarizing chunks:  84%|████████▍ | 658/783 [13:52<01:45,  1.19it/s]Your max_length is set to 130, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Summarizing chunks:  85%|████████▍ | 665/783 [13:59<01:49,  1.08it/s]Your max_length is set to 130, but your input_length is only 84. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Summarizing chunks:  85%|████████▌ | 668/783 [14:02<02:09,  1.13s/it]Your max_length is set to 130, but your input_length is only 50. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Summarizing chunks:  87%|████████▋ | 678/783 [14:13<02:07,  1.21s/it]Your max_length is set to 130, but your input_length is only 75. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Summarizing chunks:  88%|████████▊ | 691/783 [14:26<01:33,  1.02s/it]Your max_length is set to 130, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Summarizing chunks:  89%|████████▊ | 694/783 [14:29<01:27,  1.02it/s]Your max_length is set to 130, but your input_length is only 25. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
      "Summarizing chunks:  89%|████████▉ | 697/783 [14:32<01:21,  1.05it/s]Your max_length is set to 130, but your input_length is only 37. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n",
      "Summarizing chunks:  89%|████████▉ | 700/783 [14:35<01:37,  1.18s/it]Your max_length is set to 130, but your input_length is only 54. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Summarizing chunks:  90%|████████▉ | 702/783 [14:38<01:37,  1.21s/it]Your max_length is set to 130, but your input_length is only 61. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
      "Summarizing chunks:  91%|█████████ | 709/783 [14:45<01:26,  1.17s/it]Your max_length is set to 130, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Summarizing chunks:  92%|█████████▏| 720/783 [14:58<01:13,  1.16s/it]Your max_length is set to 130, but your input_length is only 61. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
      "Summarizing chunks:  92%|█████████▏| 721/783 [14:59<01:02,  1.01s/it]Your max_length is set to 130, but your input_length is only 86. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
      "Summarizing chunks:  93%|█████████▎| 726/783 [15:05<01:07,  1.18s/it]Your max_length is set to 130, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
      "Summarizing chunks:  93%|█████████▎| 730/783 [15:10<01:12,  1.37s/it]Your max_length is set to 130, but your input_length is only 55. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Summarizing chunks:  93%|█████████▎| 732/783 [15:11<00:53,  1.05s/it]Your max_length is set to 130, but your input_length is only 102. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n",
      "Summarizing chunks:  94%|█████████▎| 733/783 [15:13<01:03,  1.27s/it]Your max_length is set to 130, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Summarizing chunks:  95%|█████████▌| 745/783 [15:27<00:40,  1.08s/it]Your max_length is set to 130, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Summarizing chunks:  96%|█████████▌| 749/783 [15:31<00:37,  1.09s/it]Your max_length is set to 130, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Summarizing chunks:  96%|█████████▋| 754/783 [15:39<00:50,  1.74s/it]Your max_length is set to 130, but your input_length is only 104. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n",
      "Summarizing chunks:  97%|█████████▋| 758/783 [15:43<00:30,  1.24s/it]Your max_length is set to 130, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Summarizing chunks:  97%|█████████▋| 762/783 [15:48<00:27,  1.32s/it]Your max_length is set to 130, but your input_length is only 79. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Summarizing chunks:  98%|█████████▊| 766/783 [15:54<00:25,  1.53s/it]Your max_length is set to 130, but your input_length is only 115. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n",
      "Summarizing chunks:  98%|█████████▊| 771/783 [16:02<00:20,  1.68s/it]Your max_length is set to 130, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "Summarizing chunks:  99%|█████████▊| 773/783 [16:06<00:16,  1.68s/it]Your max_length is set to 130, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n",
      "Summarizing chunks: 100%|██████████| 783/783 [16:19<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File: Dataset summaries and citations.docx | Page: None | Chunk: 1\n",
      "🔹 Original:\n",
      " Table 1. Description of studies included in the meta-analysis. Full article citations are listed after the table.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Citation List\n",
      "Acuña E., A. A., Pastenes V., C., & Villalobos G., L. (2017). Ca ...\n",
      "Summary:\n",
      " full article citations are listed after the table . Acua E., A. A., Pastenes V., C., & Villalobos G. (2018). carbon sequestration and photosynthesis in newly established Turfgrass Cover in central Chile . \n",
      "--------------------------------------------------\n",
      "\n",
      "File: Dataset summaries and citations.docx | Page: None | Chunk: 2\n",
      "🔹 Original:\n",
      " https://doi.org/10.3390/f5030425\n",
      "Carley, D. S., Goodman, D., Sermons, S., Shi, W., Bowman, D., Miller, G., & Rufty, T. (2011). Soil Organic Matter Accumulation in Creeping Bentgrass Greens: A Chronose ...\n",
      "Summary:\n",
      " soil organic matter Accumulation in Creeping Bentgrass Greens: a Chronosequence with Implications for management and carbon sequestration . Agronomy Journal, 103(3), 604–610 . \n",
      "--------------------------------------------------\n",
      "\n",
      "File: Dataset summaries and citations.docx | Page: None | Chunk: 3\n",
      "🔹 Original:\n",
      " bock, Texas. Agronomy Journal, 112(1), 148–157. https://doi.org/10.1002/agj2.20023\n",
      "Golubiewski, N. E. (2006). Urbanization Increases Grassland Carbon Pools: Effects Of Landscaping In Colorado’s Front  ...\n",
      "Summary:\n",
      " urbanization increases Grassland carbon pools: effects of landscaping in Colorado’s front range . a comparison of soil carbon dynamics in residential yards with and without trees . \n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "summarized_chunks = summarize_document_chunks(chunked_docs)\n",
    "\n",
    "# Print a few samples\n",
    "for s in summarized_chunks[:3]:\n",
    "    print(f\"\\nFile: {s['filename']} | Page: {s.get('page')} | Chunk: {s['chunk_number']}\")\n",
    "    print(\"🔹 Original:\\n\", s['text'][:200], \"...\")\n",
    "    print(\"Summary:\\n\", s['summary'], \"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51a7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating super summaries:  20%|██        | 2/10 [00:02<00:10,  1.27s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1421 > 512). Running this sequence through the model will result in indexing errors\n",
      "Generating super summaries:  50%|█████     | 5/10 [00:17<00:21,  4.38s/it]"
     ]
    }
   ],
   "source": [
    "super_summaries = generate_super_summaries_per_file(summarized_chunks)\n",
    "\n",
    "for filename, summary in super_summaries.items():\n",
    "    print(f\"\\n📁 {filename}\")\n",
    "    print(\"🧠 Super-Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6367e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summaries = [chunk['summary'] for chunk in summarized_chunks]\n",
    "mega_summary = generate_super_summary(all_summaries)\n",
    "\n",
    "print(\"🌐 Final Mega-Summary Across All Docs:\\n\", mega_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Saving\n",
    "with open(\"summarized_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summarized_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Loading\n",
    "df = pd.DataFrame(summarized_chunks)\n",
    "df.to_csv(\"summarized_chunks.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0337f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>page</th>\n",
       "      <th>chunk_number</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dataset summaries and citations.docx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Table 1. Description of studies included in th...</td>\n",
       "      <td>full article citations are listed after the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dataset summaries and citations.docx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>https://doi.org/10.3390/f5030425\\nCarley, D. S...</td>\n",
       "      <td>soil organic matter Accumulation in Creeping B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataset summaries and citations.docx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>bock, Texas. Agronomy Journal, 112(1), 148–157...</td>\n",
       "      <td>urbanization increases Grassland carbon pools:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dataset summaries and citations.docx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>20(1), 87–96. https://doi.org/10.1007/s11252-0...</td>\n",
       "      <td>urban ecosystems, 17(1), 205–219. https://doi....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dataset summaries and citations.docx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2010). Soil Organic Carbon Input from Urban Tu...</td>\n",
       "      <td>Soil Organic Carbon Input from Urban Turfgrass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>The_Plan_of_the_Giza_Pyramids.pdf</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3</td>\n",
       "      <td>successors,  and the mathematical nature of th...</td>\n",
       "      <td>the real scientist will recognise the impossib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>The_Plan_of_the_Giza_Pyramids.pdf</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>The Plan of the Giza Pyramids  16 \\nIf, as the...</td>\n",
       "      <td>the plan of the Giza pyramids 16 is the bed-ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>The_Plan_of_the_Giza_Pyramids.pdf</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>masonry of the Great Pyramid in no way invalid...</td>\n",
       "      <td>king who wished to claim ownership of the monu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>The_Plan_of_the_Giza_Pyramids.pdf</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3</td>\n",
       "      <td>concerning the delivery of stones to the site ...</td>\n",
       "      <td>leading Egyptianologists have long recognised ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>The_Plan_of_the_Giza_Pyramids.pdf</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>are mostly deeply buried beneath the silt of t...</td>\n",
       "      <td>the Great Sphinx still stands as the enduring ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>783 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename  page  chunk_number  \\\n",
       "0    Dataset summaries and citations.docx   NaN             1   \n",
       "1    Dataset summaries and citations.docx   NaN             2   \n",
       "2    Dataset summaries and citations.docx   NaN             3   \n",
       "3    Dataset summaries and citations.docx   NaN             4   \n",
       "4    Dataset summaries and citations.docx   NaN             5   \n",
       "..                                    ...   ...           ...   \n",
       "778     The_Plan_of_the_Giza_Pyramids.pdf  15.0             3   \n",
       "779     The_Plan_of_the_Giza_Pyramids.pdf  16.0             1   \n",
       "780     The_Plan_of_the_Giza_Pyramids.pdf  16.0             2   \n",
       "781     The_Plan_of_the_Giza_Pyramids.pdf  16.0             3   \n",
       "782     The_Plan_of_the_Giza_Pyramids.pdf  16.0             4   \n",
       "\n",
       "                                                  text  \\\n",
       "0    Table 1. Description of studies included in th...   \n",
       "1    https://doi.org/10.3390/f5030425\\nCarley, D. S...   \n",
       "2    bock, Texas. Agronomy Journal, 112(1), 148–157...   \n",
       "3    20(1), 87–96. https://doi.org/10.1007/s11252-0...   \n",
       "4    2010). Soil Organic Carbon Input from Urban Tu...   \n",
       "..                                                 ...   \n",
       "778  successors,  and the mathematical nature of th...   \n",
       "779  The Plan of the Giza Pyramids  16 \\nIf, as the...   \n",
       "780  masonry of the Great Pyramid in no way invalid...   \n",
       "781  concerning the delivery of stones to the site ...   \n",
       "782  are mostly deeply buried beneath the silt of t...   \n",
       "\n",
       "                                               summary  \n",
       "0    full article citations are listed after the ta...  \n",
       "1    soil organic matter Accumulation in Creeping B...  \n",
       "2    urbanization increases Grassland carbon pools:...  \n",
       "3    urban ecosystems, 17(1), 205–219. https://doi....  \n",
       "4    Soil Organic Carbon Input from Urban Turfgrass...  \n",
       "..                                                 ...  \n",
       "778  the real scientist will recognise the impossib...  \n",
       "779  the plan of the Giza pyramids 16 is the bed-ro...  \n",
       "780  king who wished to claim ownership of the monu...  \n",
       "781  leading Egyptianologists have long recognised ...  \n",
       "782  the Great Sphinx still stands as the enduring ...  \n",
       "\n",
       "[783 rows x 5 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf05c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834725a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q2_K - Medium\n",
      "print_info: file size   = 2.63 GiB (3.35 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q2_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  2694.32 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openpyxl\\reader\\workbook.py:118: UserWarning: Print area cannot be set to Defined name: 'Wedding budget'!$A:$K.\n",
      "  warn(f\"Print area cannot be set to Defined name: {defn.value}.\")\n",
      "Summarizing:   1%|          | 7/783 [00:09<16:20,  1.26s/it]Your max_length is set to 100, but your input_length is only 98. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n",
      "Summarizing:   1%|▏         | 10/783 [00:15<22:00,  1.71s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Summarizing:   2%|▏         | 12/783 [00:18<22:42,  1.77s/it]Your max_length is set to 100, but your input_length is only 63. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n",
      "Summarizing:   4%|▎         | 28/783 [00:48<24:16,  1.93s/it]Your max_length is set to 100, but your input_length is only 24. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
      "Summarizing:  16%|█▌        | 125/783 [02:44<11:18,  1.03s/it]Your max_length is set to 100, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
      "Summarizing:  17%|█▋        | 130/783 [02:50<12:41,  1.17s/it]Your max_length is set to 100, but your input_length is only 66. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      "Summarizing:  19%|█▉        | 151/783 [03:20<18:27,  1.75s/it]Your max_length is set to 100, but your input_length is only 99. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n",
      "Summarizing:  20%|█▉        | 156/783 [03:27<16:10,  1.55s/it]Your max_length is set to 100, but your input_length is only 43. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "Summarizing:  22%|██▏       | 174/783 [03:47<10:49,  1.07s/it]Your max_length is set to 100, but your input_length is only 18. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n",
      "Summarizing:  23%|██▎       | 182/783 [03:58<13:43,  1.37s/it]Your max_length is set to 100, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Summarizing:  25%|██▍       | 194/783 [04:15<14:18,  1.46s/it]Your max_length is set to 100, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Summarizing:  27%|██▋       | 213/783 [04:45<14:58,  1.58s/it]Your max_length is set to 100, but your input_length is only 80. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Summarizing:  28%|██▊       | 223/783 [04:58<12:53,  1.38s/it]Your max_length is set to 100, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Summarizing:  30%|██▉       | 232/783 [05:12<13:53,  1.51s/it]Your max_length is set to 100, but your input_length is only 62. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n",
      "Summarizing:  30%|███       | 236/783 [05:17<12:59,  1.42s/it]Your max_length is set to 100, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Summarizing:  31%|███       | 241/783 [05:25<13:43,  1.52s/it]Your max_length is set to 100, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Summarizing:  34%|███▍      | 267/783 [06:06<13:08,  1.53s/it]Your max_length is set to 100, but your input_length is only 41. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n",
      "Summarizing:  35%|███▌      | 276/783 [06:18<11:36,  1.37s/it]Your max_length is set to 100, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n",
      "Summarizing:  41%|████      | 320/783 [07:25<10:25,  1.35s/it]Your max_length is set to 100, but your input_length is only 60. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
      "Summarizing:  42%|████▏     | 327/783 [07:34<10:24,  1.37s/it]Your max_length is set to 100, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Summarizing:  43%|████▎     | 334/783 [07:43<09:29,  1.27s/it]Your max_length is set to 100, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n",
      "Summarizing:  45%|████▌     | 353/783 [08:06<08:58,  1.25s/it]Your max_length is set to 100, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "Summarizing:  49%|████▉     | 385/783 [08:47<08:56,  1.35s/it]Your max_length is set to 100, but your input_length is only 57. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Summarizing:  53%|█████▎    | 412/783 [09:16<06:47,  1.10s/it]Your max_length is set to 100, but your input_length is only 35. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Summarizing:  54%|█████▍    | 424/783 [09:31<07:14,  1.21s/it]Your max_length is set to 100, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Summarizing:  55%|█████▍    | 428/783 [09:37<08:55,  1.51s/it]Your max_length is set to 100, but your input_length is only 71. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      "Summarizing:  55%|█████▍    | 430/783 [09:40<08:59,  1.53s/it]Your max_length is set to 100, but your input_length is only 76. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n",
      "Summarizing:  59%|█████▊    | 459/783 [10:20<06:06,  1.13s/it]Your max_length is set to 100, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Summarizing:  59%|█████▉    | 463/783 [10:24<06:05,  1.14s/it]Your max_length is set to 100, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Summarizing:  60%|██████    | 472/783 [10:36<06:24,  1.24s/it]Your max_length is set to 100, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Summarizing:  61%|██████    | 476/783 [10:41<06:51,  1.34s/it]Your max_length is set to 100, but your input_length is only 79. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Summarizing:  62%|██████▏   | 487/783 [10:58<08:08,  1.65s/it]Your max_length is set to 100, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n",
      "Summarizing:  63%|██████▎   | 497/783 [11:12<06:39,  1.40s/it]Your max_length is set to 100, but your input_length is only 85. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Summarizing:  64%|██████▍   | 502/783 [11:18<05:46,  1.23s/it]Your max_length is set to 100, but your input_length is only 92. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n",
      "Summarizing:  65%|██████▍   | 508/783 [11:25<05:22,  1.17s/it]Your max_length is set to 100, but your input_length is only 68. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      "Summarizing:  66%|██████▌   | 515/783 [11:33<05:51,  1.31s/it]Your max_length is set to 100, but your input_length is only 85. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Summarizing:  66%|██████▋   | 520/783 [11:38<04:39,  1.06s/it]Your max_length is set to 100, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      "Summarizing:  68%|██████▊   | 531/783 [11:49<04:19,  1.03s/it]Your max_length is set to 100, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Summarizing:  68%|██████▊   | 534/783 [11:52<04:20,  1.04s/it]Your max_length is set to 100, but your input_length is only 35. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Summarizing:  70%|██████▉   | 545/783 [12:05<04:33,  1.15s/it]Your max_length is set to 100, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Summarizing:  70%|███████   | 552/783 [12:12<04:07,  1.07s/it]Your max_length is set to 100, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Summarizing:  71%|███████   | 557/783 [12:17<03:40,  1.02it/s]Your max_length is set to 100, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Summarizing:  72%|███████▏  | 566/783 [12:27<03:57,  1.09s/it]Your max_length is set to 100, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Summarizing:  73%|███████▎  | 573/783 [12:34<04:00,  1.15s/it]Your max_length is set to 100, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "Summarizing:  74%|███████▎  | 576/783 [12:37<03:55,  1.14s/it]Your max_length is set to 100, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
      "Summarizing:  75%|███████▍  | 587/783 [12:48<03:08,  1.04it/s]Your max_length is set to 100, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Summarizing:  75%|███████▌  | 590/783 [12:51<03:10,  1.01it/s]Your max_length is set to 100, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Summarizing:  77%|███████▋  | 603/783 [13:06<03:35,  1.20s/it]Your max_length is set to 100, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Summarizing:  78%|███████▊  | 612/783 [13:15<02:52,  1.01s/it]Your max_length is set to 100, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Summarizing:  79%|███████▉  | 619/783 [13:22<02:55,  1.07s/it]Your max_length is set to 100, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Summarizing:  80%|███████▉  | 626/783 [13:30<02:47,  1.07s/it]Your max_length is set to 100, but your input_length is only 15. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
      "Summarizing:  81%|████████  | 631/783 [13:34<02:33,  1.01s/it]Your max_length is set to 100, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Summarizing:  84%|████████▎ | 654/783 [14:02<02:45,  1.28s/it]Your max_length is set to 100, but your input_length is only 30. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Summarizing:  84%|████████▍ | 657/783 [14:05<02:21,  1.13s/it]Your max_length is set to 100, but your input_length is only 69. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      "Summarizing:  85%|████████▍ | 662/783 [14:10<02:14,  1.11s/it]Your max_length is set to 100, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
      "Summarizing:  85%|████████▍ | 665/783 [14:13<02:21,  1.20s/it]Your max_length is set to 100, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Summarizing:  86%|████████▌ | 672/783 [14:20<01:53,  1.03s/it]Your max_length is set to 100, but your input_length is only 16. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n",
      "Summarizing:  86%|████████▌ | 675/783 [14:23<01:50,  1.02s/it]Your max_length is set to 100, but your input_length is only 27. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
      "Summarizing:  87%|████████▋ | 680/783 [14:28<01:40,  1.02it/s]Your max_length is set to 100, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Summarizing:  88%|████████▊ | 689/783 [14:38<01:53,  1.21s/it]Your max_length is set to 100, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "Summarizing:  89%|████████▊ | 694/783 [14:44<01:53,  1.28s/it]Your max_length is set to 100, but your input_length is only 58. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Summarizing:  89%|████████▉ | 699/783 [14:49<01:29,  1.06s/it]Your max_length is set to 100, but your input_length is only 32. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n",
      "Summarizing:  90%|████████▉ | 704/783 [14:53<01:06,  1.19it/s]Your max_length is set to 100, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Summarizing:  91%|█████████ | 711/783 [15:00<01:07,  1.06it/s]Your max_length is set to 100, but your input_length is only 84. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Summarizing:  91%|█████████ | 714/783 [15:03<01:18,  1.14s/it]Your max_length is set to 100, but your input_length is only 50. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Summarizing:  92%|█████████▏| 724/783 [15:15<01:15,  1.28s/it]Your max_length is set to 100, but your input_length is only 75. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Summarizing:  94%|█████████▍| 737/783 [15:28<00:49,  1.07s/it]Your max_length is set to 100, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Summarizing:  95%|█████████▍| 740/783 [15:31<00:43,  1.01s/it]Your max_length is set to 100, but your input_length is only 25. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
      "Summarizing:  95%|█████████▍| 743/783 [15:34<00:38,  1.03it/s]Your max_length is set to 100, but your input_length is only 37. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n",
      "Summarizing:  95%|█████████▌| 746/783 [15:38<00:45,  1.22s/it]Your max_length is set to 100, but your input_length is only 54. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Summarizing:  96%|█████████▌| 748/783 [15:40<00:43,  1.23s/it]Your max_length is set to 100, but your input_length is only 61. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
      "Summarizing:  96%|█████████▋| 755/783 [15:47<00:33,  1.20s/it]Your max_length is set to 100, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Summarizing:  98%|█████████▊| 766/783 [16:01<00:20,  1.21s/it]Your max_length is set to 100, but your input_length is only 61. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
      "Summarizing:  98%|█████████▊| 767/783 [16:02<00:16,  1.04s/it]Your max_length is set to 100, but your input_length is only 86. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
      "Summarizing:  99%|█████████▊| 772/783 [16:08<00:13,  1.21s/it]Your max_length is set to 100, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
      "Summarizing:  99%|█████████▉| 776/783 [16:13<00:09,  1.39s/it]Your max_length is set to 100, but your input_length is only 55. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Summarizing:  99%|█████████▉| 779/783 [16:16<00:05,  1.29s/it]Your max_length is set to 100, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Summarizing: 100%|██████████| 783/783 [16:20<00:00,  1.25s/it]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (43949 > 512). Running this sequence through the model will result in indexing errors\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\blocks.py\", line 1663, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16828\\1474620746.py\", line 193, in generate_super_summary\n",
      "    return self.summarize_chunk(\" \".join(summaries), max_length=150, min_length=50)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16828\\1474620746.py\", line 177, in summarize_chunk\n",
      "    return self.summarizer(prompt, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\text2text_generation.py\", line 280, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\text2text_generation.py\", line 173, in __call__\n",
      "    result = super().__call__(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\base.py\", line 1379, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\base.py\", line 1386, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\base.py\", line 1286, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\text2text_generation.py\", line 202, in _forward\n",
      "    output_ids = self.model.generate(**model_inputs, **generate_kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py\", line 2280, in generate\n",
      "    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py\", line 778, in _prepare_encoder_decoder_kwargs_for_generation\n",
      "    model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)  # type: ignore\n",
      "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\t5\\modeling_t5.py\", line 1131, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\t5\\modeling_t5.py\", line 682, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      "                             ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\t5\\modeling_t5.py\", line 600, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      "                       ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\t5\\modeling_t5.py\", line 527, in forward\n",
      "    scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 57.56 GiB. GPU 0 has a total capacity of 4.00 GiB of which 2.56 GiB is free. Of the allocated memory 756.77 MiB is allocated by PyTorch, and 15.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "import tiktoken\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Load your local LLaMA model (adjust path & params)\n",
    "llama_model = Llama(\n",
    "    model_path=\"llama-2-7b-chat.Q2_K.gguf\",  # path to your .gguf file\n",
    "    n_ctx=2048,\n",
    "    n_threads=8  # adjust based on your CPU\n",
    ")\n",
    "\n",
    "\n",
    "class DrXResearchAssistant:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.index = None\n",
    "        self.metadata = []\n",
    "        self.chunked_docs = []\n",
    "        self.translation_models = {}\n",
    "\n",
    "        # Summarization model\n",
    "        model_path = \"google-t5/t5-small\"\n",
    "        self.summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model_path,\n",
    "    tokenizer=model_path,\n",
    "    # device=-1  # 👈 this ensures it runs on CPU\n",
    ")\n",
    "\n",
    "\n",
    "    # ---------- FILE READING ----------\n",
    "    def read_pdf(self, file_path):\n",
    "        reader = PdfReader(file_path)\n",
    "        return [{'filename': os.path.basename(file_path), 'page': i + 1, 'text': p.extract_text()} for i, p in enumerate(reader.pages) if p.extract_text()]\n",
    "\n",
    "    def read_docx(self, file_path):\n",
    "        doc = Document(file_path)\n",
    "        text = '\\n'.join([p.text for p in doc.paragraphs])\n",
    "        return [{'filename': os.path.basename(file_path), 'text': text}]\n",
    "\n",
    "    def read_excel(self, file_path):\n",
    "        data = pd.read_excel(file_path, sheet_name=None)\n",
    "        text = '\\n'.join([df.to_string(index=False) for df in data.values()])\n",
    "        return [{'filename': os.path.basename(file_path), 'text': text}]\n",
    "\n",
    "    def read_csv(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        return [{'filename': os.path.basename(file_path), 'text': df.to_string(index=False)}]\n",
    "\n",
    "    def extract_text(self, file_path):\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        if ext == '.pdf':\n",
    "            return self.read_pdf(file_path)\n",
    "        elif ext == '.docx':\n",
    "            return self.read_docx(file_path)\n",
    "        elif ext in ['.xlsx', '.xls', '.xlsm']:\n",
    "            return self.read_excel(file_path)\n",
    "        elif ext == '.csv':\n",
    "            return self.read_csv(file_path)\n",
    "        return []\n",
    "\n",
    "    # ---------- CHUNKING ----------\n",
    "    def chunk_text(self, text, max_tokens=300, overlap=50):\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        chunks = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            chunk = self.tokenizer.decode(tokens[i:i + max_tokens])\n",
    "            chunks.append(chunk.strip())\n",
    "            i += max_tokens - overlap\n",
    "        return chunks\n",
    "\n",
    "    def chunk_documents(self, parsed_docs):\n",
    "        self.chunked_docs.clear()\n",
    "        for doc in parsed_docs:\n",
    "            text = doc['text']\n",
    "            chunks = self.chunk_text(text)\n",
    "            for idx, chunk in enumerate(chunks):\n",
    "                self.chunked_docs.append({\n",
    "                    'filename': doc['filename'],\n",
    "                    'page': doc.get('page', 'N/A'),\n",
    "                    'chunk_number': idx + 1,\n",
    "                    'text': chunk\n",
    "                })\n",
    "\n",
    "    # ---------- EMBEDDINGS + INDEX ----------\n",
    "    def embed_chunks(self):\n",
    "        texts = [c['text'] for c in self.chunked_docs]\n",
    "        embeddings = self.embed_model.encode(texts, convert_to_numpy=True)\n",
    "        dim = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.index.add(np.array(embeddings).astype('float32'))\n",
    "        self.metadata = self.chunked_docs.copy()\n",
    "\n",
    "    def search_index(self, query, top_k=5):\n",
    "        q_embed = self.embed_model.encode([query], convert_to_numpy=True)\n",
    "        D, I = self.index.search(np.array(q_embed).astype('float32'), top_k)\n",
    "        return [self.metadata[i] for i in I[0]]\n",
    "\n",
    "    # ---------- Q&A ----------\n",
    "    def build_prompt(self, question, chunks):\n",
    "        context = \"\\n\\n\".join([f\"[{c['filename']} - page {c.get('page')}]:\\n{c['text']}\" for c in chunks])\n",
    "        return f\"\"\"You are an assistant helping analyze research papers.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Sources:\n",
    "{context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    def answer_question(self, question):\n",
    "        if not self.index:\n",
    "            return \"Please upload and process documents first.\"\n",
    "\n",
    "        top_chunks = self.search_index(question, top_k=5)\n",
    "        prompt = self.build_prompt(question, top_chunks)\n",
    "\n",
    "        # Actual LLaMA response\n",
    "        output = llama_model(prompt, max_tokens=300, stop=[\"Question:\", \"User:\"], echo=False)\n",
    "        return output['choices'][0]['text'].strip()\n",
    "\n",
    "\n",
    "    # ---------- TRANSLATION ----------\n",
    "    def detect_lang(self, text):\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except:\n",
    "            return 'unknown'\n",
    "\n",
    "    def load_translation_model(self, src, tgt):\n",
    "        key = f\"{src}-{tgt}\"\n",
    "        if key not in self.translation_models:\n",
    "            model_name = f\"opus-mt-{src}-{tgt}\"\n",
    "            model = MarianMTModel.from_pretrained(model_name)\n",
    "            tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "            self.translation_models[key] = (model, tokenizer)\n",
    "        return self.translation_models[key]\n",
    "\n",
    "    def auto_translate(self, text):\n",
    "        lang = self.detect_lang(text)\n",
    "        if lang == 'ar':\n",
    "            src, tgt = 'ar', 'en'\n",
    "        elif lang == 'en':\n",
    "            src, tgt = 'en', 'ar'\n",
    "        else:\n",
    "            return f\"Unsupported language detected: {lang}\"\n",
    "\n",
    "        model, tokenizer = self.load_translation_model(src, tgt)\n",
    "        inputs = tokenizer.prepare_seq2seq_batch([text], return_tensors=\"pt\")\n",
    "        output = model.generate(**inputs)\n",
    "        translated = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "        return f\"[{src} → {tgt}]\\n{translated}\"\n",
    "\n",
    "    # ---------- SUMMARIZATION ----------\n",
    "    def summarize_chunk(self, text, max_length=100, min_length=30):\n",
    "        prompt = \"summarize: \" + text\n",
    "        return self.summarizer(prompt, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']\n",
    "\n",
    "    def summarize_chunks(self):\n",
    "        summaries = []\n",
    "        for chunk in tqdm(self.chunked_docs, desc=\"Summarizing\"):\n",
    "            try:\n",
    "                s = self.summarize_chunk(chunk['text'])\n",
    "            except:\n",
    "                s = \"[Failed to summarize]\"\n",
    "            summaries.append(s)\n",
    "        return summaries\n",
    "\n",
    "    def generate_super_summary(self):\n",
    "        if not self.chunked_docs:\n",
    "            return \"No data to summarize.\"\n",
    "        summaries = self.summarize_chunks()\n",
    "        return self.summarize_chunk(\" \".join(summaries), max_length=150, min_length=50)\n",
    "\n",
    "\n",
    "# ---------- GRADIO UI ----------\n",
    "assistant = DrXResearchAssistant()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# 🤖 Dr. X Research Assistant\")\n",
    "    gr.Markdown(\"Upload research files, ask questions, translate, and summarize — all offline.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(file_types=[\".pdf\", \".docx\", \".xlsx\", \".xls\", \".csv\"], file_count=\"multiple\", label=\"📁 Upload Files\")\n",
    "        upload_btn = gr.Button(\"Process Files\")\n",
    "        upload_output = gr.Textbox(label=\"Upload Log\")\n",
    "\n",
    "    with gr.Tab(\"❓ Ask a Question\"):\n",
    "        q_input = gr.Textbox(label=\"Enter your question\")\n",
    "        q_btn = gr.Button(\"Answer\")\n",
    "        q_output = gr.Textbox(label=\"Answer\")\n",
    "\n",
    "    with gr.Tab(\"🌍 Translate\"):\n",
    "        t_input = gr.Textbox(label=\"Enter text to translate\")\n",
    "        t_btn = gr.Button(\"Translate\")\n",
    "        t_output = gr.Textbox(label=\"Translated Text\")\n",
    "\n",
    "    with gr.Tab(\"📝 Super Summary\"):\n",
    "        s_btn = gr.Button(\"Generate Super Summary\")\n",
    "        s_output = gr.Textbox(label=\"Summary\", lines=10)\n",
    "\n",
    "    def process_files(files):\n",
    "        all_chunks = []\n",
    "        for f in files:\n",
    "            chunks = assistant.extract_text(f.name)\n",
    "            all_chunks.extend(chunks)\n",
    "        assistant.chunk_documents(all_chunks)\n",
    "        assistant.embed_chunks()\n",
    "        return f\"✅ Processed {len(assistant.chunked_docs)} chunks from {len(files)} file(s).\"\n",
    "\n",
    "    upload_btn.click(fn=process_files, inputs=[file_input], outputs=[upload_output])\n",
    "    q_btn.click(fn=assistant.answer_question, inputs=[q_input], outputs=[q_output])\n",
    "    t_btn.click(fn=assistant.auto_translate, inputs=[t_input], outputs=[t_output])\n",
    "    s_btn.click(fn=assistant.generate_super_summary, outputs=[s_output])\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
